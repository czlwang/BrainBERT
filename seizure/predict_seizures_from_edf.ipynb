{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd627ea2-4bcb-4316-98c0-564c7d889512",
   "metadata": {},
   "source": [
    "# SEEG Seizure Detection Script\n",
    "\n",
    "This script processes SEEG data stored in EDF files to detect seizures using a pre-trained BrainBERT model and a logistic regression classifier. The script imports necessary libraries and custom modules, processes the EDF data, generates embeddings using BrainBERT, and predicts seizures using a logistic regression model.\n",
    "\n",
    "## Code Overview\n",
    "\n",
    "### Importing Libraries and Modules\n",
    "\n",
    "The script begins by importing essential libraries such as `os`, `sys`, `numpy`, `torch`, and `joblib`, along with `OmegaConf` for configuration handling. It also imports custom functions from various modules related to BrainBERT and SEEG preprocessing.\n",
    "\n",
    "### Setting the Working Directory\n",
    "\n",
    "The working directory is set to the BrainBERT directory to ensure that the script has access to the necessary files and modules.\n",
    "\n",
    "### Function to Process an EDF File and Make Predictions\n",
    "\n",
    "A function is defined to:\n",
    "- Preprocess the EDF file to create epochs and save them.\n",
    "- Load the preprocessed data from the output directory.\n",
    "- Generate BrainBERT embeddings for the preprocessed data.\n",
    "- Use the logistic regression model to predict seizures.\n",
    "- Identify and return epochs and channels where seizures are detected.\n",
    "\n",
    "### Loading the BrainBERT Model\n",
    "\n",
    "The BrainBERT model is loaded from a specified checkpoint path.\n",
    "\n",
    "### Loading the Logistic Regression Model\n",
    "\n",
    "The trained logistic regression model is loaded from a joblib file.\n",
    "\n",
    "### Defining Paths\n",
    "\n",
    "Paths for the EDF directory, epoch output directory, and concatenated output directory are defined to organize the processing workflow.\n",
    "\n",
    "### Processing the EDF File\n",
    "\n",
    "The main script processes the EDF file using the defined function, identifies seizure epochs and channels, and prints the results.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "The script demonstrates how to use the defined functions to detect seizures in an EDF file and output the epochs and channels where seizures are detected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c17256e3-5494-45c7-b356-8a36d4b34119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "Changed directory and updated sys.path...\n",
      "Imported custom functions and models...\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries and Modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "print(\"Starting script...\")\n",
    "\n",
    "# Set the working directory to the BrainBERT directory\n",
    "os.chdir('/home/vineetreddy/Dropbox/CZW_MIT/BrainBERT')\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)  # Add the parent directory to the system path\n",
    "\n",
    "print(\"Changed directory and updated sys.path...\")\n",
    "\n",
    "#from train_brainbert_logreg import *\n",
    "from preprocess_edf_pipeline import *  # Import custom preprocessing functions\n",
    "from demo_brainbert_annotated import *  # Import custom functions\n",
    "import models  # Custom models\n",
    "\n",
    "print(\"Imported custom functions and models...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e41cf69-faa9-4ca6-a98e-75972128a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load pre-trained model weights and configuration\n",
    "def load_brainbert_model(ckpt_path):\n",
    "    \"\"\"\n",
    "    Loads the BrainBERT model with pre-trained weights.\n",
    "\n",
    "    Args:\n",
    "        ckpt_path (str): Path to the checkpoint file.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: BrainBERT model with loaded weights.\n",
    "    \"\"\"\n",
    "    cfg = OmegaConf.create({\"upstream_ckpt\": ckpt_path})\n",
    "    brainbert_model = build_model(cfg)  # Build the model with the given configuration\n",
    "    brainbert_model.to('cuda')  # Move the model to GPU\n",
    "    init_state = torch.load(ckpt_path)  # Load the initial state of the model\n",
    "    load_model_weights(brainbert_model, init_state['model'], False)  # Load the model weights\n",
    "    return brainbert_model\n",
    "\n",
    "# Function to generate BrainBERT embeddings from example waveforms\n",
    "def generate_brainbert_embeddings(model, example_wavs):\n",
    "    \"\"\"\n",
    "    Generates BrainBERT embeddings for each example.\n",
    "\n",
    "    Args:\n",
    "        brainbert_model (torch.nn.Module): BrainBERT model.\n",
    "        example_wavs (np.array): Array of example waveforms.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of BrainBERT embeddings.\n",
    "    \"\"\"\n",
    "    brainbert_outs = []\n",
    "    for example_wav in example_wavs:\n",
    "        # Get the Short-Time Fourier Transform (STFT) of the signal\n",
    "        f, t, linear = get_stft(example_wav, 2048, clip_fs=25, nperseg=400, noverlap=350, normalizing=\"zscore\", return_onesided=True)\n",
    "        inputs = torch.FloatTensor(linear).unsqueeze(0).transpose(1, 2).to('cuda')  # Prepare inputs for the model\n",
    "        mask = torch.zeros((inputs.shape[:2])).bool().to('cuda')  # Create a mask for the inputs\n",
    "        with torch.no_grad():\n",
    "            out = brainbert_model.forward(inputs, mask, intermediate_rep=True)  # Get the model output\n",
    "        brainbert_outs.append(out.cpu().numpy())  # Append the output to the list\n",
    "\n",
    "    # Concatenate and average the outputs\n",
    "    brainbert_outs_arrr = np.concatenate(brainbert_outs, axis=0)\n",
    "    brainbert_outs_arr = brainbert_outs_arrr.mean(axis=1)\n",
    "    \n",
    "    return brainbert_outs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b6a0089-b05d-42bf-9bfd-ff6066cfac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined process_edf_file function...\n",
      "Loading BrainBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vineetreddy/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrainBERT model loaded...\n",
      "Loading logistic regression model...\n",
      "Logistic regression model loaded...\n",
      "Defined paths...\n",
      "Starting to process the EDF file...\n",
      "Entered process_edf_file function...\n",
      "Starting preprocessing...\n",
      "Extracting EDF parameters from /home/vineetreddy/edf predict/sub-HUP060_ses-presurgery_task-ictal_acq-seeg_run-01_ieeg.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 188999  =      0.000 ...   377.998 secs...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up high-pass filter at 0.1 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal highpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.10\n",
      "- Lower transition bandwidth: 0.10 Hz (-6 dB cutoff frequency: 0.05 Hz)\n",
      "- Filter length: 16501 samples (33.002 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 1691 samples (6.605 s)\n",
      "\n",
      "Not setting metadata\n",
      "75 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 75 events and 1280 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Process the EDF file and get the epochs and channels with seizures\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to process the EDF file...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m seizure_epochs_channels \u001b[38;5;241m=\u001b[39m process_edf_file(seizure_prediction_edf_dir, brainbert_model, logistic_model, epoch_output_dir, concat_output_dir)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished processing the EDF file...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mprocess_edf_file\u001b[0;34m(edf_file_path, model, logistic_model, epoch_output_dir, concat_output_dir)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting preprocessing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m preprocess_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m process_directory(edf_file_path, epoch_output_dir, concat_output_dir)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mpreprocess_start_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load the preprocessed data\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/CZW_MIT/BrainBERT/vineet/preprocess_edf_pipeline.py:236\u001b[0m, in \u001b[0;36mprocess_directory\u001b[0;34m(input_dir, epoch_output_dir, concat_output_dir)\u001b[0m\n\u001b[1;32m    233\u001b[0m             process_file(filepath, epoch_output_dir)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Step 2: Concatenate epochs for each channel\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m concatenate_epochs(epoch_output_dir, concat_output_dir)\n",
      "File \u001b[0;32m~/Dropbox/CZW_MIT/BrainBERT/vineet/preprocess_edf_pipeline.py:191\u001b[0m, in \u001b[0;36mconcatenate_epochs\u001b[0;34m(epoch_output_dir, concat_output_dir)\u001b[0m\n\u001b[1;32m    188\u001b[0m epoch_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Load the numpy array from the file\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m epoch_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(filepath)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# If the channel is not already in the dictionary, add it\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m channels_data:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:434\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    432\u001b[0m _ZIP_SUFFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x05\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# empty zip files start with this\u001b[39;00m\n\u001b[1;32m    433\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX)\n\u001b[0;32m--> 434\u001b[0m magic \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread(N)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to process an EDF file and make predictions\n",
    "def process_edf_file(edf_file_path, model, logistic_model, epoch_output_dir, concat_output_dir):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Start of the function\n",
    "    print(\"Entered process_edf_file function...\")\n",
    "\n",
    "    # Preprocess the EDF file to create epochs and save them\n",
    "    print(\"Starting preprocessing...\")\n",
    "    preprocess_start_time = time.time()\n",
    "    process_directory(edf_file_path, epoch_output_dir, concat_output_dir)\n",
    "    print(f\"Preprocessing took {time.time() - preprocess_start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Load the preprocessed data\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    load_start_time = time.time()\n",
    "    example_wavs = []\n",
    "    for filename in os.listdir(concat_output_dir):\n",
    "        if filename.endswith('.npy'):\n",
    "            filepath = os.path.join(concat_output_dir, filename)\n",
    "            data = np.load(filepath)\n",
    "            example_wavs.append(data)\n",
    "    example_wavs = np.array(example_wavs)\n",
    "    print(f\"Loading preprocessed data took {time.time() - load_start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Generate BrainBERT embeddings\n",
    "    print(\"Generating BrainBERT embeddings...\")\n",
    "    embedding_start_time = time.time()\n",
    "    brainbert_outs_arr = generate_brainbert_embeddings(model, example_wavs)\n",
    "    print(f\"Generating embeddings took {time.time() - embedding_start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Predict using the logistic regression model\n",
    "    print(\"Making predictions...\")\n",
    "    prediction_start_time = time.time()\n",
    "    predictions = logistic_model.predict(brainbert_outs_arr)\n",
    "    print(f\"Making predictions took {time.time() - prediction_start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Identify epochs and channels with seizures\n",
    "    print(\"Identifying seizures...\")\n",
    "    seizure_identification_start_time = time.time()\n",
    "    seizure_epochs_channels = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        if pred == 1:\n",
    "            channel_name = os.path.basename(concat_output_dir).split('_')[0]\n",
    "            seizure_epochs_channels.append((i, channel_name))\n",
    "    print(f\"Identifying seizures took {time.time() - seizure_identification_start_time:.2f} seconds.\")\n",
    "    \n",
    "    print(f\"Total processing time: {time.time() - start_time:.2f} seconds.\")\n",
    "    \n",
    "    return seizure_epochs_channels\n",
    "\n",
    "print(\"Defined process_edf_file function...\")\n",
    "\n",
    "# Load the BrainBERT model\n",
    "print(\"Loading BrainBERT model...\")\n",
    "ckpt_path = \"/home/vineetreddy/Dropbox/CZW_MIT/stft_large_pretrained_256hz.pth\"\n",
    "brainbert_model = load_brainbert_model(ckpt_path)\n",
    "print(\"BrainBERT model loaded...\")\n",
    "\n",
    "# Load the trained logistic regression model from a joblib file\n",
    "print(\"Loading logistic regression model...\")\n",
    "logistic_model_path = \"/home/vineetreddy/save model/logistic_model.joblib\" #path to logistic model joblib file\n",
    "logistic_model = joblib.load(logistic_model_path)\n",
    "print(\"Logistic regression model loaded...\")\n",
    "\n",
    "# Define the paths\n",
    "seizure_prediction_edf_dir = '/home/vineetreddy/edf predict'\n",
    "epoch_output_dir = '/home/vineetreddy/epoch output dir'\n",
    "concat_output_dir = '/home/vineetreddy/concat output dir'\n",
    "\n",
    "print(\"Defined paths...\")\n",
    "\n",
    "# Process the EDF file and get the epochs and channels with seizures\n",
    "print(\"Starting to process the EDF file...\")\n",
    "seizure_epochs_channels = process_edf_file(seizure_prediction_edf_dir, brainbert_model, logistic_model, epoch_output_dir, concat_output_dir)\n",
    "print(\"Finished processing the EDF file...\")\n",
    "\n",
    "# Print the results\n",
    "for epoch, channel in seizure_epochs_channels:\n",
    "    print(f\"Seizure detected in epoch {epoch} on channel {channel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e37fe5-9155-461f-a55c-80107ef59f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
